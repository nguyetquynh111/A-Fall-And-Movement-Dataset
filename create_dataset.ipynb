{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "bf6add2b",
      "metadata": {
        "id": "bf6add2b"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import pandas as pd\n",
        "import cv2\n",
        "import tensorflow as tf\n",
        "import numpy as np\n",
        "import glob\n",
        "from tqdm import tqdm\n",
        "import re\n",
        "from sklearn.model_selection import train_test_split\n",
        "import pickle\n",
        "import tensorflow as tf"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "LABELS = [\"ForwardFall\", \"BackwardFall\", \"LeftFall\", \"RightFall\", \"GetDown\", \"SitDown\", \"Walk\"]\n",
        "LABEL2INDEX = dict(zip(LABELS, range(len(LABELS))))\n",
        "INDEX2LABEL = dict(zip(range(len(LABELS)), LABELS))\n",
        "\n",
        "IMAGE_WIDTH = 32\n",
        "IMAGE_HEIGHT = 32"
      ],
      "metadata": {
        "id": "0nypqt2KF4Dp"
      },
      "id": "0nypqt2KF4Dp",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "9049e907",
      "metadata": {
        "id": "9049e907"
      },
      "outputs": [],
      "source": [
        "def get_all_paths(directory):\n",
        "    files = []\n",
        "    for root, _, filenames in os.walk(directory):\n",
        "        for filename in filenames:\n",
        "            if filename.endswith('.mp4') or filename.endswith('.MOV'):\n",
        "                files.append(os.path.join(root, filename))\n",
        "    return files\n",
        "\n",
        "def get_subject(file):\n",
        "    return re.findall(\"(Subject\\d).*\",file)[0]"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def count_frames(video_path):\n",
        "    cap = cv2.VideoCapture(video_path)\n",
        "    if not cap.isOpened():\n",
        "        print(\"Error: Could not open video.\")\n",
        "        return -1\n",
        "\n",
        "    total_frames = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))\n",
        "    return total_frames"
      ],
      "metadata": {
        "id": "QtqphbIHn9ma"
      },
      "id": "QtqphbIHn9ma",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def count_all_frames(files):\n",
        "  frames = 0\n",
        "  for file in files:\n",
        "    frame = count_frames(file)\n",
        "    frames+=frame\n",
        "  return frames"
      ],
      "metadata": {
        "id": "Hl6vg8dXnt26"
      },
      "id": "Hl6vg8dXnt26",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "015fabe4",
      "metadata": {
        "id": "015fabe4",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "7758945d-4308-45e2-b0f7-49b5c4926a43"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ForwardFall\n",
            "52\n",
            "15478\n",
            "BackwardFall\n",
            "52\n",
            "15525\n",
            "LeftFall\n",
            "50\n",
            "15553\n",
            "RightFall\n",
            "49\n",
            "15548\n",
            "GetDown\n",
            "72\n",
            "15517\n",
            "SitDown\n",
            "65\n",
            "15534\n",
            "Walk\n",
            "52\n",
            "15600\n"
          ]
        }
      ],
      "source": [
        "directory =  'ViFam'\n",
        "df = pd.DataFrame()\n",
        "\n",
        "for label in LABELS:\n",
        "  path = os.path.join(directory, label)\n",
        "  files = sorted(get_all_paths(path))\n",
        "  print(label)\n",
        "  print(len(files))\n",
        "  frames = count_all_frames(files)\n",
        "  print(frames)\n",
        "  sub_df = pd.DataFrame({\"name\": files})\n",
        "  sub_df['label'] = label\n",
        "  if len(df)==0:\n",
        "      df = sub_df\n",
        "  else:\n",
        "      df = pd.concat([df, sub_df],axis=0)\n",
        "df = df.sample(len(df))\n",
        "X = df[[\"name\"]]\n",
        "y = df[[\"label\"]]\n",
        "X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.4, random_state=42, stratify=y)\n",
        "X_val, X_test, y_val, y_test = train_test_split(X_val, y_val, test_size=0.4, random_state=42, stratify=y_val)\n",
        "\n",
        "df_train = pd.concat([X_train, y_train], axis=1)\n",
        "df_train.to_csv(\"df_train.csv\", index=False)\n",
        "df_val = pd.concat([X_val, y_val], axis=1)\n",
        "df_val.to_csv(\"df_val.csv\", index=False)\n",
        "df_test = pd.concat([X_test, y_test], axis=1)\n",
        "df_test.to_csv(\"df_test.csv\", index=False)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "len(df_train), len(df_val), len(df_test)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "b4_Odqx0kvh_",
        "outputId": "3396fa34-7ad8-4137-b8af-c26948ad8ba1"
      },
      "id": "b4_Odqx0kvh_",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(235, 94, 63)"
            ]
          },
          "metadata": {},
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Create datasets"
      ],
      "metadata": {
        "id": "35ZQMkOrprNe"
      },
      "id": "35ZQMkOrprNe"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "5e4113d1",
      "metadata": {
        "id": "5e4113d1"
      },
      "outputs": [],
      "source": [
        "def create_dataset(df, text: str):\n",
        "    all_frames_count = 0\n",
        "    for filename in tqdm(df.name.values):\n",
        "        video = cv2.VideoCapture(filename)\n",
        "        frames_count = video.get(cv2.CAP_PROP_FRAME_COUNT)\n",
        "        all_frames_count+=frames_count\n",
        "\n",
        "    images = np.zeros((int(all_frames_count), IMAGE_HEIGHT, IMAGE_WIDTH, 1), dtype=np.float32)\n",
        "    labels = np.zeros(int(all_frames_count))\n",
        "    count = 0\n",
        "    for index, row in tqdm(df.iterrows()):\n",
        "        filename, label = row\n",
        "        vid = cv2.VideoCapture(filename)\n",
        "        success, frame = vid.read()\n",
        "        while success:\n",
        "            frame = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY)\n",
        "            image = frame\n",
        "            frame = cv2.resize(frame, (IMAGE_HEIGHT, IMAGE_WIDTH))\n",
        "            frame = tf.keras.utils.img_to_array(frame)\n",
        "            frame = tf.expand_dims(frame, 0)\n",
        "            images[count] = frame\n",
        "            labels[count] = LABEL2INDEX[label]\n",
        "            count+=1\n",
        "            success, frame = vid.read()\n",
        "    dataset = tf.data.Dataset.from_tensor_slices((images, labels))\n",
        "    tf.data.Dataset.save(\n",
        "      dataset, text, compression='GZIP'\n",
        "  )\n",
        "    with open(text + '/element_spec', 'wb') as out_:\n",
        "        pickle.dump(dataset.element_spec, out_)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "31e06762",
      "metadata": {
        "id": "31e06762",
        "outputId": "06463dd2-fd93-4553-a1ca-71acd7c44305",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 235/235 [00:02<00:00, 106.34it/s]\n",
            "235it [18:19,  4.68s/it]\n"
          ]
        }
      ],
      "source": [
        "df = df_train\n",
        "text = \"train\"\n",
        "create_dataset(df, text)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "868ad910",
      "metadata": {
        "id": "868ad910",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "52e2366b-0369-4838-8677-e01e64a11ca0"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 63/63 [00:01<00:00, 60.73it/s]\n",
            "63it [04:07,  3.92s/it]\n"
          ]
        }
      ],
      "source": [
        "df = df_test\n",
        "text = \"test\"\n",
        "create_dataset(df, text)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "758b0963",
      "metadata": {
        "id": "758b0963",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "99ea17ad-afa6-45e5-fc69-d5a58a62643d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 94/94 [00:01<00:00, 70.75it/s]\n",
            "94it [07:23,  4.72s/it]\n"
          ]
        }
      ],
      "source": [
        "df = df_val\n",
        "text = \"val\"\n",
        "create_dataset(df, text)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Create datasets for 3D-CNN model"
      ],
      "metadata": {
        "id": "CangpuLyp0nb"
      },
      "id": "CangpuLyp0nb"
    },
    {
      "cell_type": "code",
      "source": [
        "def create_overlap_dataset(df: pd.DataFrame, img_width: int, img_height: int):\n",
        "    \"\"\"Tạo video frame từ các video có path nằm trong df. Mỗi video frame gồm 36 frames liên tiếp nhau.\"\"\"\n",
        "\n",
        "    video_frames = [] # list contain all video frames\n",
        "    labels = [] # list contain labels of video frames\n",
        "\n",
        "    for index, row in tqdm(df.iterrows()): # loop each video\n",
        "        filename, label = row\n",
        "        vid = cv2.VideoCapture(filename)\n",
        "        end_frame = int(vid.get(cv2.CAP_PROP_FRAME_COUNT))\n",
        "        success, frame = vid.read()\n",
        "        frame_count = 0\n",
        "\n",
        "        # save overlap video frames in 1 video\n",
        "        clip_1 = []\n",
        "        clip_2 = []\n",
        "        clip_3 = []\n",
        "        clip_4 = []\n",
        "        clip_5 = []\n",
        "        clip_6 = []\n",
        "        clip_7 = []\n",
        "        clip_8 = []\n",
        "        clip_9 = []\n",
        "        clip_10 = []\n",
        "        clip_11 = []\n",
        "        clip_12 = []\n",
        "        while success:\n",
        "            frame_count += 1\n",
        "            frame = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY)\n",
        "            resized = cv2.resize(frame, (img_width, img_height), interpolation = cv2.INTER_NEAREST)\n",
        "\n",
        "            clip_1.append(resized)\n",
        "            if (frame_count > 3):\n",
        "                clip_2.append(resized)\n",
        "            if (frame_count > 6):\n",
        "                clip_3.append(resized)\n",
        "            if (frame_count > 9):\n",
        "                clip_4.append(resized)\n",
        "            if (frame_count > 12):\n",
        "                clip_5.append(resized)\n",
        "            if (frame_count > 15):\n",
        "                clip_6.append(resized)\n",
        "            if (frame_count > 18):\n",
        "                clip_7.append(resized)\n",
        "            if (frame_count > 21):\n",
        "                clip_8.append(resized)\n",
        "            if (frame_count > 24):\n",
        "                clip_9.append(resized)\n",
        "            if (frame_count > 27):\n",
        "                clip_10.append(resized)\n",
        "            if (frame_count > 30):\n",
        "                clip_11.append(resized)\n",
        "            if (frame_count > 33):\n",
        "                clip_12.append(resized)\n",
        "\n",
        "            if (frame_count % 36 == 0):\n",
        "                video_frames.append(clip_1)\n",
        "                labels.append(LABEL2INDEX[label])\n",
        "                clip_1 = []\n",
        "            if (frame_count % 36 == 3 and frame_count > 3):\n",
        "                video_frames.append(clip_2)\n",
        "                labels.append(LABEL2INDEX[label])\n",
        "                clip_2 = []\n",
        "            if (frame_count % 36 == 6 and frame_count > 6):\n",
        "                video_frames.append(clip_3)\n",
        "                labels.append(LABEL2INDEX[label])\n",
        "                clip_3 = []\n",
        "            if (frame_count % 36 == 9 and frame_count > 9):\n",
        "                video_frames.append(clip_4)\n",
        "                labels.append(LABEL2INDEX[label])\n",
        "                clip_4 = []\n",
        "            if (frame_count % 36 == 12 and frame_count > 12):\n",
        "                video_frames.append(clip_5)\n",
        "                labels.append(LABEL2INDEX[label])\n",
        "                clip_5 = []\n",
        "            if (frame_count % 36 == 15 and frame_count > 15):\n",
        "                video_frames.append(clip_6)\n",
        "                labels.append(LABEL2INDEX[label])\n",
        "                clip_6 = []\n",
        "            if (frame_count % 36 == 18 and frame_count > 18):\n",
        "                video_frames.append(clip_7)\n",
        "                labels.append(LABEL2INDEX[label])\n",
        "                clip_7 = []\n",
        "            if (frame_count % 36 == 21 and frame_count > 21):\n",
        "                video_frames.append(clip_8)\n",
        "                labels.append(LABEL2INDEX[label])\n",
        "                clip_8 = []\n",
        "            if (frame_count % 36 == 24 and frame_count > 24):\n",
        "                video_frames.append(clip_9)\n",
        "                labels.append(LABEL2INDEX[label])\n",
        "                clip_9 = []\n",
        "            if (frame_count % 36 == 27 and frame_count > 27):\n",
        "                video_frames.append(clip_10)\n",
        "                labels.append(LABEL2INDEX[label])\n",
        "                clip_10 = []\n",
        "            if (frame_count % 36 == 30 and frame_count > 30):\n",
        "                video_frames.append(clip_11)\n",
        "                labels.append(LABEL2INDEX[label])\n",
        "                clip_11 = []\n",
        "            if (frame_count % 36 == 33 and frame_count > 33):\n",
        "                video_frames.append(clip_12)\n",
        "                labels.append(LABEL2INDEX[label])\n",
        "                clip_12 = []\n",
        "            success, frame = vid.read()\n",
        "        vid.release()\n",
        "\n",
        "    video_frames = np.array(video_frames)\n",
        "    print(video_frames.shape)\n",
        "    labels = np.array(labels)\n",
        "    print(labels.shape)\n",
        "\n",
        "    return video_frames, labels\n",
        "\n",
        "def save_3dcnn_dataset(dir: str, video_frames: np.array, labels: np.array):\n",
        "    os.makedirs(dir, exist_ok = True)\n",
        "    dataset = tf.data.Dataset.from_tensor_slices((video_frames, labels))\n",
        "    tf.data.Dataset.save(dataset, dir, compression='GZIP')\n",
        "    with open(dir + '/element_spec', 'wb') as out_:\n",
        "        pickle.dump(dataset.element_spec, out_)"
      ],
      "metadata": {
        "id": "6ODbTq36qKeA"
      },
      "id": "6ODbTq36qKeA",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "os.mkdir(\"cnn3d\")\n",
        "video_frames, labels = create_overlap_dataset(df_train, IMAGE_WIDTH, IMAGE_HEIGHT)\n",
        "save_3dcnn_dataset(os.path.join(\"cnn3d\", \"train\"), video_frames, labels)\n",
        "video_frames, labels = create_overlap_dataset(df_val, IMAGE_WIDTH, IMAGE_HEIGHT)\n",
        "save_3dcnn_dataset(os.path.join(new_path, \"val\"), video_frames, labels)\n",
        "video_frames, labels = create_overlap_dataset(df_test, IMAGE_WIDTH, IMAGE_HEIGHT)\n",
        "save_3dcnn_dataset(os.path.join(new_path, \"test\"), video_frames, labels)"
      ],
      "metadata": {
        "id": "yfGAyylsqUVE"
      },
      "id": "yfGAyylsqUVE",
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.0"
    },
    "colab": {
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}